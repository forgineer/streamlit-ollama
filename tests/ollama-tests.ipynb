{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabb8d7e",
   "metadata": {},
   "source": [
    "# Ollama Test Notebook\n",
    "\n",
    "This notebook is meant to demo the capabilities of the ollama Python library and making various requests to an Ollama server installed locally or externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30156f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import ChatResponse, Client, ShowResponse\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "ollama_client = Client(\n",
    "    host=\"http://kryten3:11434\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510bb61",
   "metadata": {},
   "source": [
    "## Send a chat message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2754e46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: deepseek-r1:8b\n",
      "created_at: 2026-01-17T03:05:13.882621716Z\n",
      "done: True\n",
      "done_reason: stop\n",
      "total_duration: 7871878744\n",
      "load_duration: 220360024\n",
      "prompt_eval_count: 8\n",
      "prompt_eval_duration: 120868522\n",
      "eval_count: 214\n",
      "eval_duration: 7435927631\n",
      "message: role='assistant' content='Hello! üòä Great to hear from you. How can I help you today?' thinking=\"Okay, the user just said ‚ÄúHello, Ollama!‚Äù in a very friendly and straightforward way. \\n\\nHmm, this seems like a casual greeting, probably from someone who's just starting to interact with me. They might be testing the waters, curious about what I can do, or maybe they have a specific question in mind but haven't gotten to it yet. \\n\\nGiven the tone is positive and open-ended, I should match that energy with a warm, helpful response. No need to dive into technical details unless they ask‚Äîjust keep it simple and inviting. \\n\\nI'll greet them back with a smiley to keep it friendly, mention that I'm here to help, and leave the door open for them to ask whatever comes to mind. The key is to be approachable and encourage further interaction. \\n\\nUser probably isn't looking for anything too complex here‚Äîthey just want to know I'm alive and ready. Keeping it light should work well.\\n\" images=None tool_name=None tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "chat_response: ChatResponse = ollama_client.chat(model=\"deepseek-r1:8b\", messages=[{\"role\": \"user\", \"content\": \"Hello, Ollama!\"}], think=True)\n",
    "\n",
    "#pprint(chat_response)\n",
    "for key, value in chat_response:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac205be5",
   "metadata": {},
   "source": [
    "## List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b6833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(model='deepseek-r1:8b', modified_at=datetime.datetime(2026, 1, 16, 20, 25, 1, 460025, tzinfo=TzInfo(-21600)), digest='6995872bfe4c521a67b32da386cd21d5c6e819b6e0d62f79f64ec83be99f5763', size=5225376047, details=ModelDetails(parent_model='', format='gguf', family='qwen3', families=['qwen3'], parameter_size='8.2B', quantization_level='Q4_K_M')),\n",
      " Model(model='functiongemma:latest', modified_at=datetime.datetime(2025, 12, 22, 22, 33, 6, 129344, tzinfo=TzInfo(-21600)), digest='7c19b650567acfb1bee50d12bb286370e8a54d8877f6a0ecfd01f8c8fdc223bb', size=300807157, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='268.10M', quantization_level='Q8_0')),\n",
      " Model(model='granite3.3:8b', modified_at=datetime.datetime(2025, 11, 4, 23, 0, 6, 29948, tzinfo=TzInfo(-21600)), digest='fd429f23b90980ed1bef53b990894e7b0199331f6ae90c5650240a7d5b70f1f7', size=4942891653, details=ModelDetails(parent_model='', format='gguf', family='granite', families=['granite'], parameter_size='8.2B', quantization_level='Q4_K_M')),\n",
      " Model(model='codegemma:7b', modified_at=datetime.datetime(2025, 10, 6, 20, 49, 41, 113308, tzinfo=TzInfo(-18000)), digest='0c96700aaada572ce9bb6999d1fda9b53e9e6cef5d74fda1e066a1ba811b93f3', size=5011852809, details=ModelDetails(parent_model='', format='gguf', family='gemma', families=['gemma'], parameter_size='9B', quantization_level='Q4_0')),\n",
      " Model(model='gemma3:12b', modified_at=datetime.datetime(2025, 10, 5, 18, 11, 32, 371827, tzinfo=TzInfo(-18000)), digest='f4031aab637d1ffa37b42570452ae0e4fad0314754d17ded67322e4b95836f8a', size=8149190253, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='12.2B', quantization_level='Q4_K_M'))]\n"
     ]
    }
   ],
   "source": [
    "models: list = ollama_client.list()\n",
    "pprint(models['models'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084d06e",
   "metadata": {},
   "source": [
    "## Show model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3c7565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_at: 2026-01-16 20:25:01.460025-06:00\n",
      "template: {{- if .System }}{{ .System }}{{ end }}\n",
      "{{- range $i, $_ := .Messages }}\n",
      "{{- $last := eq (len (slice $.Messages $i)) 1}}\n",
      "{{- if eq .Role \"user\" }}<ÔΩúUserÔΩú>{{ .Content }}\n",
      "{{- else if eq .Role \"assistant\" }}<ÔΩúAssistantÔΩú>\n",
      "  {{- if and $.IsThinkSet (and $last .Thinking) -}}\n",
      "<think>\n",
      "{{ .Thinking }}\n",
      "</think>\n",
      "{{- end }}{{ .Content }}{{- if not $last }}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>{{- end }}\n",
      "{{- end }}\n",
      "{{- if and $last (ne .Role \"assistant\") }}<ÔΩúAssistantÔΩú>\n",
      "{{- if and $.IsThinkSet (not $.Think) -}}\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{{ end }}\n",
      "{{- end -}}\n",
      "{{- end }}\n",
      "modelfile: # Modelfile generated by \"ollama show\"\n",
      "# To build a new Modelfile based on this, replace FROM with:\n",
      "# FROM deepseek-r1:8b\n",
      "\n",
      "FROM /usr/share/ollama/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d\n",
      "TEMPLATE \"\"\"{{- if .System }}{{ .System }}{{ end }}\n",
      "{{- range $i, $_ := .Messages }}\n",
      "{{- $last := eq (len (slice $.Messages $i)) 1}}\n",
      "{{- if eq .Role \"user\" }}<ÔΩúUserÔΩú>{{ .Content }}\n",
      "{{- else if eq .Role \"assistant\" }}<ÔΩúAssistantÔΩú>\n",
      "  {{- if and $.IsThinkSet (and $last .Thinking) -}}\n",
      "<think>\n",
      "{{ .Thinking }}\n",
      "</think>\n",
      "{{- end }}{{ .Content }}{{- if not $last }}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>{{- end }}\n",
      "{{- end }}\n",
      "{{- if and $last (ne .Role \"assistant\") }}<ÔΩúAssistantÔΩú>\n",
      "{{- if and $.IsThinkSet (not $.Think) -}}\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{{ end }}\n",
      "{{- end -}}\n",
      "{{- end }}\"\"\"\n",
      "PARAMETER stop <ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>\n",
      "PARAMETER stop <ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "PARAMETER stop <ÔΩúUserÔΩú>\n",
      "PARAMETER stop <ÔΩúAssistantÔΩú>\n",
      "PARAMETER temperature 0.6\n",
      "PARAMETER top_p 0.95\n",
      "LICENSE \"\"\"MIT License\n",
      "\n",
      "Copyright (c) 2023 DeepSeek\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\"\"\"\n",
      "\n",
      "license: MIT License\n",
      "\n",
      "Copyright (c) 2023 DeepSeek\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n",
      "details: parent_model='' format='gguf' family='qwen3' families=['qwen3'] parameter_size='8.2B' quantization_level='Q4_K_M'\n",
      "modelinfo: {'general.architecture': 'qwen3', 'general.basename': 'DeepSeek-R1-0528-Qwen3', 'general.file_type': 15, 'general.license': 'mit', 'general.parameter_count': 8190735360, 'general.quantization_version': 2, 'general.size_label': '8B', 'general.type': 'model', 'qwen3.attention.head_count': 32, 'qwen3.attention.head_count_kv': 8, 'qwen3.attention.key_length': 128, 'qwen3.attention.layer_norm_rms_epsilon': 1e-06, 'qwen3.attention.value_length': 128, 'qwen3.block_count': 36, 'qwen3.context_length': 131072, 'qwen3.embedding_length': 4096, 'qwen3.feed_forward_length': 12288, 'qwen3.rope.freq_base': 1000000, 'qwen3.rope.scaling.factor': 4, 'qwen3.rope.scaling.original_context_length': 32768, 'qwen3.rope.scaling.type': 'yarn', 'tokenizer.ggml.add_bos_token': False, 'tokenizer.ggml.add_eos_token': False, 'tokenizer.ggml.bos_token_id': 151643, 'tokenizer.ggml.eos_token_id': 151645, 'tokenizer.ggml.merges': None, 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.padding_token_id': 151645, 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.ggml.token_type': None, 'tokenizer.ggml.tokens': None}\n",
      "parameters: stop                           \"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>\"\n",
      "stop                           \"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\"\n",
      "stop                           \"<ÔΩúUserÔΩú>\"\n",
      "stop                           \"<ÔΩúAssistantÔΩú>\"\n",
      "temperature                    0.6\n",
      "top_p                          0.95\n",
      "capabilities: ['completion', 'thinking']\n"
     ]
    }
   ],
   "source": [
    "show_response: ShowResponse = ollama_client.show('deepseek-r1:8b')\n",
    "\n",
    "for key, value in show_response:    \n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "#print(show_response['capabilities'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe335f2d",
   "metadata": {},
   "source": [
    "## List model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0eaa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_info = show_response['modelinfo']\n",
    "    \n",
    "    for key, value in model_info.items():\n",
    "        print(f\"- {key}: {value}\")\n",
    "except KeyError:\n",
    "    print(\"No info found for this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06d3af",
   "metadata": {},
   "source": [
    "## List model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c594826",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_parameters = show_response['parameters'].split('\\n')\n",
    "    \n",
    "    for param in model_parameters:\n",
    "        print(f\"- {param.split()}\")\n",
    "except KeyError:\n",
    "    print(\"No parameters found for this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5d349",
   "metadata": {},
   "source": [
    "## List capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_capabilities = show_response['capabilities']\n",
    "    \n",
    "    for capability in model_capabilities:\n",
    "        print(f\"- {capability.strip()}\")\n",
    "except KeyError:\n",
    "    print(\"No capabilities found for this model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit-ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
